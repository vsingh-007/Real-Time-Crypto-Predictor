# -*- coding: utf-8 -*-
"""Real_Time_Crypto_Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwpX3k13-0xwVx46XWJnYN4fO1w4Y-Vg

"""

pip install ydata_profiling

pip install ta

import requests
import pandas as pd
import numpy as np
import logging
import os
import joblib
import shap
from scipy import stats
from ta.trend import SMAIndicator, EMAIndicator
from ta.momentum import RSIIndicator
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from ydata_profiling import ProfileReport
import warnings
warnings.filterwarnings("ignore")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("crypto_model.log"),
        logging.StreamHandler()
    ]
)

MODEL_PATH = "crypto_model.pkl"

# 1. Fetch CryptoCompare Data
def fetch_crypto_data(symbol="BTC", currency="USD", limit=1095):
    try:
        logging.info(f"Fetching real-time data for {symbol}...")
        url = "https://min-api.cryptocompare.com/data/v2/histoday"
        params = {"fsym": symbol, "tsym": currency, "limit": limit}
        response = requests.get(url, params=params)
        data = response.json()

        if "Data" not in data or "Data" not in data["Data"]:
            raise ValueError("API response is malformed.")

        df = pd.DataFrame(data["Data"]["Data"])
        df["time"] = pd.to_datetime(df["time"], unit="s")
        logging.info(f"Data fetched successfully for {symbol}!")
        return df
    except Exception as e:
        logging.error(f"Error fetching crypto data for {symbol}: {e}")
        raise

df = fetch_crypto_data(symbol="BTC", currency="USD", limit=1460)

# 2. Feature Engineering
def create_features(df):
    try:
        logging.info("Creating features...")
        df["SMA_10"] = SMAIndicator(close=df["close"], window=10).sma_indicator()
        df["EMA_10"] = EMAIndicator(close=df["close"], window=10).ema_indicator()
        df["RSI_14"] = RSIIndicator(close=df["close"], window=14).rsi()
        df["day"] = df["time"].dt.day
        df["month"] = df["time"].dt.month
        df["year"] = df["time"].dt.year
        df["dayofweek"] = df["time"].dt.dayofweek
        df["target"] = df["close"].shift(-1)
        df.dropna(inplace=True)
        logging.info("Features created.")
        return df
    except Exception as e:
        logging.error(f"Error in feature engineering: {e}")
        raise

global features
features = ["open", "high", "low", "volumefrom", "volumeto", "SMA_10", "EMA_10", "RSI_14", "day", "month", "year", "dayofweek"]
df = create_features(df)

def explore_crypto_data(df):
  # Generate the EDA report
  global profile
  profile = ProfileReport(df, title="Real Time Crypto Prediction EDA Report")
  return profile

# Calling EDA report for statstical analysis

# Fetch data
df = fetch_crypto_data()

# Feature Engineering
df = create_features(df)

# Exploring features
explore_crypto_data(df)
profile

# 7. Plot Correlation Matrix
def plot_correlation_matrix(df):
    plt.figure(figsize=(10, 6))
    sns.heatmap(df.drop(columns=["time", "conversionType", "conversionSymbol"]).corr(), annot=True, cmap="coolwarm")
    plt.title("Correlation Matrix of Features")
    plt.tight_layout()
    plt.show()

plot_correlation_matrix(df)

def plot_univariate_distributions(df, features):

    #Plots histograms and KDE plots for each feature in the dataset.

    num_features = len(features)
    ncols = 3
    nrows = (num_features + ncols - 1) // ncols

    plt.figure(figsize=(18, 5 * nrows))
    for i, feature in enumerate(features):
        plt.subplot(nrows, ncols, i + 1)
        sns.histplot(df[feature], kde=True, bins=30, color="blue")
        plt.title(f"Distribution of {feature}")
        plt.xlabel(feature)
        plt.ylabel("Frequency")
    plt.tight_layout()
    plt.suptitle("Univariate Feature Distributions", fontsize=16, y=1.02)
    plt.show()

plot_univariate_distributions(df, features[:-4])

def plot_bivariate_relationships(df, features, target="target"):

    #Plots bivariate scatter plots of each feature vs the target variable with regression line.

    assert isinstance(target, str), "'target' must be a column name (string)."

    num_features = len(features)
    ncols = 3
    nrows = (num_features + ncols - 1) // ncols

    plt.figure(figsize=(18, 5 * nrows))
    for i, feature in enumerate(features):
        plt.subplot(nrows, ncols, i + 1)
        sns.regplot(data=df, x=feature, y=target, scatter_kws={"alpha": 0.3}, line_kws={"color": "red"})
        plt.title(f"{feature} vs {target}")
        plt.xlabel(feature)
        plt.ylabel(target)
    plt.tight_layout()
    plt.suptitle("Bivariate Feature-Target Relationships", fontsize=16, y=1.02)
    plt.show()

plot_bivariate_relationships(df, features, target="target")

# 3. Train or Retrain Models (LightGBM, XGBoost, RandomForest)
def train_models(df, retrain=False):
    features = ["open", "high", "low", "volumefrom", "volumeto", "SMA_10", "EMA_10", "RSI_14", "day", "month", "year", "dayofweek"]
    X = df[features]
    y = df["target"]
    tscv = TimeSeriesSplit(n_splits=5)
    for train_index, test_index in tscv.split(X):
      X_train, X_val = X.iloc[train_index], X.iloc[test_index]
      y_train, y_val = y.iloc[train_index], y.iloc[test_index]
    models = {}

    if not retrain and os.path.exists(MODEL_PATH):
        logging.info("Loading existing models...")
        models = joblib.load(MODEL_PATH)
    else:
        logging.info("Training models...")
        # LightGBM
        lgbm = LGBMRegressor(
            objective='regression',
            learning_rate=0.005,
            num_leaves=50,
            max_depth=7,
            min_data_in_leaf=30,
            lambda_l1=0.1,
            lambda_l2=0.1,
            feature_fraction=0.9,
            bagging_fraction=0.9,
            bagging_freq=5,
            n_estimators=500
        )

        grid = GridSearchCV(LGBMRegressor(), {"n_estimators": [100, 300, 500]}, cv=3)
        grid.fit(X_train, y_train)
        model_lgb = grid.best_estimator_
        models['LightGBM'] = model_lgb

        # XGBoost
        xgb = XGBRegressor(
            objective='reg:squarederror',
            learning_rate=0.005,
            max_depth=6,
            n_estimators=500
        )
        xgb.fit(X_train, y_train)
        models['XGBoost'] = xgb

        # RandomForest
        rf = RandomForestRegressor(
            n_estimators=500,
            max_depth=6,
            random_state=42
        )
        rf.fit(X_train, y_train)
        models['RandomForest'] = rf

        # Save models
        joblib.dump(models, MODEL_PATH)
        logging.info("Models trained and saved!")

    return models, X_val, y_val


models, X_val, y_val = train_models(df, retrain=True)

# 4. Evaluate Models
def evaluate_model(models, X_val, y_val):
    evaluation_results = {}

    try:
        for model_name, model in models.items():
            y_pred = model.predict(X_val)
            mae = mean_absolute_error(y_val, y_pred)
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            r2 = r2_score(y_val, y_pred)

            evaluation_results[model_name] = {"MAE": mae, "RMSE": rmse, "R2": r2}

            logging.info(f"   Evaluation Results for {model_name}:")
            logging.info(f"   MAE:  {mae:.2f}")
            logging.info(f"   RMSE: {rmse:.2f}")
            logging.info(f"   RÂ²:   {r2:.4f}")

            errors = y_val - y_pred
            plt.hist(errors, bins=30)
            plt.title(f"Residual Distribution - {model_name}")
            plt.show()

            plt.scatter(y_val, y_val - y_pred)
            plt.axhline(0, color='red')
            plt.title(f"Residual Scatter - {model_name}")
            plt.show()

        return evaluation_results
    except Exception as e:
        logging.error(f"Evaluation failed: {e}")
        raise


evaluate_model(models, X_val, y_val)

def feature_importances(model_lgb):
    importances = model_lgb.feature_importances_
    plt.barh(features, importances)
    plt.title("Feature Importances - LightGBM")
    plt.show()

    explainer = shap.Explainer(model_lgb)
    shap_values = explainer(X_val)
    shap.plots.beeswarm(shap_values)

feature_importances(models["LightGBM"])

# 5. Plot Actual vs Predicted for Each Model Individually
def plot_comparison_individual(models, X_val, y_val):
    # Plot LightGBM Comparison
    lgbm_pred = models['LightGBM'].predict(X_val)
    plt.figure(figsize=(8, 5))
    plt.plot(y_val.values, label='Actual', marker='o', linestyle='--')
    plt.plot(lgbm_pred, label='LightGBM Predicted', marker='x')
    plt.title("LightGBM - Actual vs Predicted Close Price (Validation Set)")
    plt.xlabel("Validation Samples")
    plt.ylabel("Price (USD)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Plot XGBoost Comparison
    xgb_pred = models['XGBoost'].predict(X_val)
    plt.figure(figsize=(8, 5))
    plt.plot(y_val.values, label='Actual', marker='o', linestyle='--')
    plt.plot(xgb_pred, label='XGBoost Predicted', marker='x')
    plt.title("XGBoost - Actual vs Predicted Close Price (Validation Set)")
    plt.xlabel("Validation Samples")
    plt.ylabel("Price (USD)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Plot RandomForest Comparison
    rf_pred = models['RandomForest'].predict(X_val)
    plt.figure(figsize=(8, 5))
    plt.plot(y_val.values, label='Actual', marker='o', linestyle='--')
    plt.plot(rf_pred, label='RandomForest Predicted', marker='x')
    plt.title("RandomForest - Actual vs Predicted Close Price (Validation Set)")
    plt.xlabel("Validation Samples")
    plt.ylabel("Price (USD)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_comparison_individual(models, X_val, y_val)

# 6. Plot Next Hour Prediction (Line Chart for Each Model)
def plot_next_hour_prediction(models, df):
    last_row = df.iloc[-1]
    last_close = last_row["close"]
    last_time = last_row["time"]
    last_features = last_row[["open", "high", "low", "volumefrom", "volumeto",
                              "SMA_10", "EMA_10", "RSI_14", "day", "month", "year", "dayofweek"]].values.reshape(1, -1)

    # Predict the next hour for each model
    next_hour_predictions = {model_name: model.predict(last_features)[0] for model_name, model in models.items()}
    next_time = last_time + pd.Timedelta(hours=1)

    def plot_model_prediction(model_name, pred_price, color):
        plt.figure(figsize=(10, 6))
        color = "green" if pred_price > last_close else "Red"
        plt.plot(df["time"].tail(10), df["close"].tail(10), label="Actual Close", marker='o')

        # Arrow showing movement
        plt.annotate('', xy=(next_time, pred_price), xytext=(last_time, last_close),
                     arrowprops=dict(facecolor=color, shrink=0.05, width=2, headwidth=8))

        plt.scatter(next_time, pred_price, color=color,label=f"{model_name} Next Hour Prediction", marker='x', s=100)
        plt.title(f"{model_name} - Next Hour Prediction")
        plt.xlabel("Time")
        plt.ylabel("Price (USD)")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    # Plot for each model
    plot_model_prediction("LightGBM", next_hour_predictions['LightGBM'], color="blue")
    plot_model_prediction("XGBoost", next_hour_predictions['XGBoost'], color="orange")
    plot_model_prediction("RandomForest", next_hour_predictions['RandomForest'], color="green")


plot_next_hour_prediction(models, df)

"""### â Conclusion & Future Scope
This project successfully built a real-time crypto classifier that can predict short-term price movement using machine learning. Future extensions could include:
- Sentiment analysis from Twitter/Reddit
- LSTM-based sequence models
- Deployment on Streamlit or Flask

**Industry Alignment:** Binary prediction aligns with trading bots and risk control strategies in crypto hedge funds.
"""
